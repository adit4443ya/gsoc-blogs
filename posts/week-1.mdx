---
title: Week 1 - Kickoff Contributions to OpenMP Support in LFortran
date: 2025-05-24
excerpt: Analyze and Design for OpenMP Support.
---


Hello, I’m Aditya Trivedi, a pre-final year B.Tech student at IIT Jodhpur, excited to kick off my Google Summer of Code (GSoC) 2025 journey with LFortran! My project focuses on enhancing OpenMP support in LFortran, a modern Fortran compiler, to enable advanced parallel programming constructs like `teams`, `tasks`, and `sections`. This blog post summarizes my progress in the first week, outlining the objective, current design, challenges, proposed solution, and insights from exploring Clang and GFortran.

## Objective

The goal is to extend LFortran’s OpenMP support beyond the current `parallel do` construct to include `teams`, `tasks`, `sections`, `single`, and `simd`, aligning with the OpenMP 6.0 specification. This will empower LFortran users to leverage high-performance computing (HPC) features for complex parallel workloads, such as hierarchical parallelism and dynamic task scheduling, making LFortran a competitive tool in the HPC community.

## Current Design

LFortran currently supports the `parallel do` construct with clauses like `private`, `shared`, `reduction`, and `collapse`. The process works as follows:

- **Parsing**: The `visit_Pragma` function in `ast_body_visitor.cpp` recognizes `!$omp parallel do` and its clauses, converting them into a `DoConcurrentLoop` node in the Abstract Semantic Representation (ASR).
- **Semantic Analysis**: Validates loop variables and clauses, ensuring correct types and declarations.
- **Backend (OpenMP Pass)**: The `openmp.cpp` pass outlines the loop body into a function, partitions iterations using `omp_get_thread_num` and `omp_get_num_threads`, and generates `GOMP_parallel` calls to the GNU OpenMP library (`libgomp`).
- **Example**: A `parallel do` loop is transformed into a `DoConcurrentLoop` node, lowered to a function with thread partitioning, as seen in [Issue #3777](https://github.com/lfortran/lfortran/issues/3777#issuecomment-2104814180).

This design is effective for loop-based parallelism but limited for other OpenMP constructs.

## Challenges in the Current Design

The `DoConcurrentLoop` approach faces several limitations when extending support to new constructs:

- **Non-Loop Constructs**: Constructs like `sections` (independent code blocks) and `tasks` (dynamic scheduling) don’t fit the loop-centric `DoConcurrentLoop` node, requiring awkward workarounds.
- **Clause Support**: New clauses (e.g., `num_teams` for `teams`, `depend` for `tasks`) are not easily integrated into the existing node structure.
- **Nesting**: Handling nested constructs (e.g., `parallel do` inside `teams`) is complex, as `DoConcurrentLoop` assumes a single loop level.
- **Maintainability**: Adapting a loop-based node for diverse constructs risks creating a convoluted design, harder to maintain as OpenMP evolves.

These challenges prompted exploration of a new design to support a broader range of OpenMP features.

## Proposed Design: `OMPRegion` ASR Node

To address these limitations, I propose introducing an `OMPRegion` ASR node, designed to handle all OpenMP constructs flexibly. The node structure is:

```plaintext
stmt
  = ...
  | OMPRegion(omp_region_type region, omp_clause* clauses, stmt* body)

omp_region_type
  = Parallel | Do | ParallelDo | Sections | Single | Task | Simd | Teams | Target | TargetData

omp_clause
  = OMPPrivate(expr* vars) | OMPShared(expr* vars) | OMPReduction(reduction_op operator, expr* vars) | 

  reduction_op
  = ReduceAdd | ReduceSub | ReduceMul | ReduceMIN | ReduceMAX

schedule_type
  = Static | Dynamic | Guided | Auto | Runtime

  ...

```

**Benefits**:
- **Flexibility**: Supports both loop-based (`parallel do`) and non-loop constructs (`sections`, `tasks`) naturally.
- **Extensibility**: Easily adds new constructs and clauses by extending `omp_region_type` and `omp_clause`, aligning with OpenMP 6.0.
- **Nesting**: Handles nested directives via recursive `OMPRegion` nodes in the body.
- **Standards Alignment**: Mirrors GFortran’s tree nodes (e.g., `OMP_SECTIONS`) and Clang’s AST classes (e.g., `OMPSectionsDirective`), facilitating `libgomp` integration.

The `OMPRegion` node will be processed in a new OpenMP pass, mapping each `region` type to appropriate GOMP calls (e.g., `GOMP_teams`, `GOMP_task`).

## Exploration of Clang and GFortran

To inform the design, I analyzed how Clang and GFortran handle OpenMP constructs:

- **GFortran (GCC)**:
  - **Frontend**: Parses directives into specific tree nodes (e.g., `OMP_TEAMS`, `OMP_SECTIONS`) with `OMP_CLAUSE` nodes for clauses.
  - **Middle-End**: Lowers to GIMPLE, outlining bodies into functions and generating `libgomp` calls (e.g., `GOMP_sections_start`).
  - **Backend**: Produces assembly with runtime calls for thread management.
  - **Tools**: Used `gfortran -fdump-tree-all` to inspect tree nodes.

- **Clang (LLVM)**:
  - **Frontend**: Creates AST classes (e.g., `OMPTeamsDirective`, `OMPTaskDirective`) with separate clause objects.
  - **Code Generation**: Outlines bodies and generates LLVM IR with `libomp` calls (e.g., `__kmpc_fork_teams`), though LFortran uses `libgomp` due to variadic function issues.
  - **Tools**: Used `clang -fopenmp -Xclang -ast-dump` to analyze ASTs and `-emit-llvm` for IR.

Both compilers use specific nodes per directive, suggesting `OMPRegion`’s flexibility is a practical compromise for LFortran’s evolving needs. Discussions are ongoing in [Issue #7332](https://github.com/lfortran/lfortran/issues/7332).

## Example: Sections Construct with and without Pragmas

To illustrate, consider the `Tasks` construct MRE from [Issue #7366](https://github.com/lfortran/lfortran/issues/7365):

### With Pragmas (Fortran)
```fortran
program parallel_processing
    use omp_lib
    implicit none

    integer, parameter :: N = 10
    integer :: i

    !$omp parallel
    !$omp single
    do i = 1, N
        !$omp task
        call process_item(i)
        !$omp end task
    end do
    !$omp end single
    !$omp end parallel

contains

    subroutine process_item(i)
        integer, intent(in) :: i
        integer :: thread_num

        thread_num = omp_get_thread_num()
        print *, "Processing item ", i, " on thread ", thread_num
    end subroutine process_item

end program parallel_processing
```

**Clang AST** (simplified, from `flang-new -fopenmp -Xclang -ast-dump`):
```plaintext
  |-OMPParallelDirective 0x5dcc678e58f0 <line:12:5, col:25>
    | `-CapturedStmt 0x5dcc678e5870 <line:13:5, line:23:5>
    |   |-CapturedDecl 0x5dcc678e44b8 <<invalid sloc>> <invalid sloc> nothrow
    |   | |-CompoundStmt 0x5dcc678e57d0 <line:13:5, line:23:5>
    |   | | `-OMPSingleDirective 0x5dcc678e5798 <line:14:9, col:27>
    |   | |   `-CapturedStmt 0x5dcc678e5738 <line:15:9, line:22:9>
    |   | |     |-CapturedDecl 0x5dcc678e4ae8 <<invalid sloc>> <invalid sloc>
    |   | |     | |-CompoundStmt 0x5dcc678e5698 <line:15:9, line:22:9>
    |   | |     | | `-ForStmt 0x5dcc678e5660 <line:16:13, line:21:13>
    |   | |     | |   |-DeclStmt 0x5dcc678e4c88 <line:16:18, col:27>
    |   | |     | |   | `-VarDecl 0x5dcc678e4c00 <col:18, col:26> col:22 used i 'int' cinit
    |   | |     | |   `-CompoundStmt 0x5dcc678e5648 <col:42, line:21:13>
    |   | |     | |     `-OMPTaskDirective 0x5dcc678e5600 <line:17:17, col:33>
    |   | |     | |       |-OMPFirstprivateClause 0x5dcc678e55c0 <<invalid sloc>> <implicit>
    |   | |     | |       `-CapturedStmt 0x5dcc678e53e0 <line:18:17, line:20:17>
    |   | |     | |         `-CapturedDecl 0x5dcc678e4f78 <<invalid sloc>> <invalid sloc> nothrow
    |   | |     | |           |-CompoundStmt 0x5dcc678e53c8 <line:18:17, line:20:17>
    |   | |     | |           | `-CallExpr 0x5dcc678e5388 <line:19:21, col:35> 'void'
    |   | |     | |           |   |-ImplicitCastExpr 0x5dcc678e5370 <col:21> 'void (*)(int)' <FunctionToPointerDecay>
    |   | |     | |           |   | `-DeclRefExpr 0x5dcc678e5300 <col:21> 'void (int)' Function 0x5dcc678e3f08 'process_item' 'void (int)'
    |   | |     | |           |   `-ImplicitCastExpr 0x5dcc678e53b0 <col:34> 'int' <LValueToRValue>
```

**Proposed LFortran ASR**:
```plaintext
OMPRegion(
  region = Parallel,
  clauses = [],
  body = [
    OMPRegion(
      region = Single,
      clauses = [],
      body = [
        DoLoop(
          head = [{v = "i", start = IntegerConstant(1), end = IntegerConstant(10)}],
          body = [
            OMPRegion(
              region = Task,
              clauses = [],
              body = [Call(symbol="process_item")]
            )
          ]
        )
      ]
    )
  ]
)
```

### Without Pragmas (Fortran with GOMP Calls)
```fortran
module thread_data_module_tasks
  use, intrinsic :: iso_c_binding
  implicit none
  type, bind(C) :: thread_data
    integer(c_int) :: i
  end type thread_data
  integer(c_long), parameter :: THREAD_DATA_SIZE = 4  ! Size of thread_data (bytes)
  integer(c_long), parameter :: THREAD_DATA_ALIGN = 4 ! Alignment of thread_data (bytes)
end module thread_data_module_tasks

module omp_lib
  use iso_c_binding
  implicit none
  interface
    subroutine GOMP_parallel(fn, data, num_threads, flags) bind(C, name="GOMP_parallel")
      import :: c_funptr, c_ptr, c_int
      type(c_funptr), value :: fn
      type(c_ptr), value :: data
      integer(c_int), value :: num_threads
      integer(c_int), value :: flags
    end subroutine
    subroutine GOMP_task(fn, data, cpyfn, arg_size, arg_align, if_clause, flags, depend) &
                         bind(C, name="GOMP_task")
      use, intrinsic :: iso_c_binding
      type(c_ptr), value :: fn, data, cpyfn, depend
      integer(c_long), value :: arg_size, arg_align
      logical(c_bool), value :: if_clause
      integer(c_int), value :: flags
    end subroutine
    function omp_get_thread_num() bind(c, name="omp_get_thread_num")
      import :: c_int
      integer(c_int) :: omp_get_thread_num
    end function
  end interface
end module omp_lib

subroutine process_item(i)
  use omp_lib
  implicit none
  integer, intent(in) :: i
  print *, "Processing item ", i, " on thread ", omp_get_thread_num()
end subroutine process_item

subroutine task_fn(data) bind(C)
  use thread_data_module_tasks
  implicit none
  type(c_ptr), value :: data
  type(thread_data), pointer :: d
  call c_f_pointer(data, d)
  call process_item(d%i)
end subroutine task_fn

subroutine parallel_region(data) bind(C)
  use thread_data_module_tasks
  use omp_lib
  implicit none
  type(c_ptr), value :: data
  integer(c_int), pointer :: n
  integer :: i
  type(thread_data), target :: task_data
  type(c_ptr) :: task_ptr

  interface
    subroutine task_fn(data) bind(C)
      use thread_data_module_tasks
      type(c_ptr), value :: data
    end subroutine task_fn
  end interface

  call c_f_pointer(data, n)
  if (omp_get_thread_num() == 0) then
    do i = 1, n
      task_data%i = i
      task_ptr = c_loc(task_data)
      call GOMP_task(c_funloc(task_fn), task_ptr, c_null_ptr, THREAD_DATA_SIZE, &
                     THREAD_DATA_ALIGN, .true._c_bool, 0, c_null_ptr)
    end do
  end if
end subroutine parallel_region

program main
  use thread_data_module_tasks
  use omp_lib
  use, intrinsic :: iso_c_binding
  implicit none
  integer, target :: n = 10
  type(c_ptr) :: ptr

  interface
    subroutine parallel_region(data) bind(C)
      use thread_data_module_tasks
      type(c_ptr), value :: data
    end subroutine parallel_region
  end interface

  ptr = c_loc(n)
  call GOMP_parallel(c_funloc(parallel_region), ptr, 0, 0)
end program main
```

**Proposed LFortran ASR**: Same as above, as the `OMPRegion` node represents the directive’s intent, lowered to GOMP calls in the backend.

## Progress and Issues

To track progress, I opened the following issues:
- [OPENMP] TEAM Construct [#7363](https://github.com/lfortran/lfortran/issues/7363)
- [OPENMP] TASK Construct [#7365](https://github.com/lfortran/lfortran/issues/7365)
- [OPENMP] SECTIONS Construct [#7366](https://github.com/lfortran/lfortran/issues/7366)

The main discussion is consolidated in [Issue #7332](https://github.com/lfortran/lfortran/issues/7332), where I shared MREs (C and Fortran, with and without pragmas) and analyzed Clang/GFortran’s OpenMP handling. These issues include detailed MREs and GOMP-based implementations to guide the design.
